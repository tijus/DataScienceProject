{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "1198\n",
      "700\n",
      "300\n",
      "866\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "from TwitterSearch import *\n",
    "import datetime\n",
    "import pandas as pd \n",
    "import time\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "# ref:\n",
    "\n",
    "for i in range(0,7):\n",
    "    try:\n",
    "        tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "        #tso.set_keywords([])\n",
    "\n",
    "        words = ['MarchForOurLives','#MarchForOurLivesdc', '#MarchForOurLivesLA', '#MarchForOurLivesBoston',\n",
    "        '@AMarch4OurLives',\"#marchforourlives\",'@AMarchForOurLives']\n",
    "\n",
    "        tso.set_keywords([words[i]]) \n",
    "        # let's define all words we would like to have a look for\n",
    "        tso.set_language('en') # we want to see English tweets only\n",
    "        tso.set_include_entities(False) # and don't give us all those entity information\n",
    "\n",
    "\n",
    "        # it's about time to create a TwitterSearch object with our secret tokens\n",
    "        ts = TwitterSearch(\n",
    "            consumer_key = '8xOVQj0CndBQQrE3ag3ck5R2k',\n",
    "            consumer_secret = 'WzJgzSUjPBBONjInO6N4TSSL9eXEGi0UxfftYZywbqGbZaZRkr',\n",
    "            access_token = '961749540079620096-pfb1dsevjN9Fitj9dn0Vo0qDwcMHuDG',\n",
    "            access_token_secret = 'ZcjVebxhlAuNey9GX5MZhv7VPzFeNCnkf09sZ41vlELq5'\n",
    "         )\n",
    "\n",
    "         # this is where the fun actually starts :)\n",
    "        i=0\n",
    "\n",
    "        tweets = []\n",
    "        for tweet in ts.search_tweets_iterable(tso):\n",
    "            i = i+1\n",
    "            if(i%100==0):\n",
    "                print (i)\n",
    "            #print( '%s=>%s' % (tweet['created_at'], tweet['text']) )\n",
    "            row = [tweet['id'] , tweet['created_at'], tweet['text']] \n",
    "            #print(row)\n",
    "            for i in range (len(row)):\n",
    "                row[i] = re.sub(',',' ',str(row[i]))\n",
    "            tweets.append(row)\n",
    "            \n",
    "        if(len(tweets) > 0):\n",
    "            print(len(tweets))\n",
    "            dfTweets = pd.DataFrame(tweets)\n",
    "            existing = pd.read_csv('Twitterdata.csv')\n",
    "            x = np.vstack([dfTweets.values, existing.values])\n",
    "            combined = (pd.DataFrame(x)).drop_duplicates()\n",
    "            combined.to_csv ('Twitterdata.csv', index = False)\n",
    "\n",
    "    except TwitterSearchException as e: # take care of all those ugly errors if there are some\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /usr/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def scrapeHTML(url):\n",
    "    text = \"\"\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page,\"html\")\n",
    "    heading = soup.h1.string\n",
    "    contents = soup.find_all(\"p\")\n",
    "    for content in contents:\n",
    "        text = text+str(content.string)+\" \"\n",
    "    heading = re.sub(',' , ' ', heading)\n",
    "    text = re.sub(',' , ' ', text)\n",
    "    Page = {'url':url, 'content':heading+ \" \" + text}\n",
    "    return Page\n",
    "\n",
    "Pages = pd.DataFrame(columns = ['url','content'])\n",
    "\n",
    "\n",
    "api_key =\"3a2aeebff29144ab99ef0fe96f89ff24\"\n",
    "query = \"March for our lives\"\n",
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=\"+api_key+\"&q=\"+query\n",
    "response = requests.get(url)\n",
    "response_data = response.content\n",
    "json_data =response_data.decode('utf8') # converting data to string\n",
    "#print(json_data);\n",
    "#ref => https://docs.python.org/3/library/json.html\n",
    "data = json.loads(json_data);\n",
    "#ref => https://docs.python.org/3/library/json.html  \n",
    "s = json.dumps(data, indent = 2, )\n",
    "\n",
    "\n",
    "data_dump = data['response']['docs']\n",
    "len(data_dump)\n",
    "\n",
    "for i in data_dump:\n",
    "    Pages = Pages.append(scrapeHTML(i['web_url']), ignore_index = True)\n",
    "    \n",
    "old = pd.read_csv('Articles1.csv', encoding = 'latin-1')\n",
    "xx = np.vstack ([old.values,Pages.values])\n",
    "ArtCombined = (pd.DataFrame(xx, columns = ['url', 'content'])).drop_duplicates()\n",
    "ArtCombined.to_csv('Articles1.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniquify():\n",
    "    # makes the 2 csvs unique\n",
    "    x = pd.read_csv('Articles1.csv', encoding = 'latin-1')\n",
    "    x = x.drop_duplicates(['url'])\n",
    "    x.to_csv('Articles2.csv')\n",
    "\n",
    "    y = pd.read_csv('Twitterdata.csv')\n",
    "    y = y.drop_duplicates(['2'])\n",
    "    y.to_csv('UniqueTwitterdata.csv')\n",
    "    \n",
    "    #print(x['content'], y['2'])\n",
    "    np.savetxt(r'tweets_final.txt', y['2'].values, fmt='%s')\n",
    "    np.savetxt(r'articles_final.txt', x['content'].values, fmt='%s')\n",
    "\n",
    "uniquify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
