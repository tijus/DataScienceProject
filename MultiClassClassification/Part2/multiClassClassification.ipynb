{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "#import featureExtractor.tfidf as tfidf\n",
    "from functools import reduce\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def readData(dataPath):\n",
    "    #input: file path \n",
    "    #output: returns a spark dataframe \n",
    "    data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(dataPath)\n",
    "    print(\"Data Read successfully ...\")\n",
    "    drop_list = ['url']\n",
    "    \n",
    "    data = data.select([column for column in data.columns if column not in drop_list])\n",
    "    return data.dropna()\n",
    "\n",
    "\n",
    "\n",
    "def tfidf(regexTokenizer, stopwordsRemover, label_stringIdx):\n",
    "    #returns pipeline using tfidf \n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "    return Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "def countVectorizer(regexTokenizer, stopwordsRemover, label_stringIdx):\n",
    "    # bag of words count\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "    label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "    return Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])   \n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    special = ['ms','mr','http','https','amp', 'none', 'i’m', 'th','don’t','it’s','advertisement']\n",
    "    with open('seostopwords.txt') as f:\n",
    "        stopLines = f.read().splitlines()\n",
    "\n",
    "    dump = [stop_words.add(i) for i in special]\n",
    "    dump = [stop_words.add(i) for i in stopLines]\n",
    "    return list(stop_words)\n",
    "    \n",
    "def classify(featureExtractor, classifier):\n",
    "    #input : \n",
    "        #1)feature extractor (count vectorizer or tfidf)\n",
    "        #2) classifier algorithm (random forest, naive bayes or linear reg)\n",
    "        \n",
    "    # training :80% , testing 10% , validation 10%\n",
    "    \n",
    "    \n",
    "    #Part 1:  Combine data from all sources \n",
    "    data_politics = readData('politics.csv')\n",
    "    data_sports = readData('sports.csv')\n",
    "    data_business = readData('business.csv')\n",
    "    data_movies = readData('movies.csv')\n",
    "    \n",
    "    \n",
    "    data_merged = data_politics.union(data_sports)\n",
    "    data_merged = data_merged.union(data_business)\n",
    "    data_merged = data_merged.union(data_movies)\n",
    "    \n",
    "    \n",
    "    data = data_merged.withColumnRenamed(\"category\", \"Category\").withColumnRenamed(\"content\", \"Descript\")\n",
    "    print(\"The total number of articles from 4 categories is \" , data.count())\n",
    "        \n",
    "    # Part 2 -> build regex tokenizer \n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "    \n",
    "    # stop words ---------- (3)\n",
    "    add_stopwords = get_stopwords()\n",
    "    #print(add_stopwords)\n",
    "    \n",
    "    # Part 3 -> build stop word tokenizer \n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\",stopWords=add_stopwords)\n",
    "    #print(stopwordsRemover.transform())\n",
    "    label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "\n",
    "    # Part 4 -> build pipeline \n",
    "    if featureExtractor==\"cv\":\n",
    "        pipeline = countVectorizer(regexTokenizer, stopwordsRemover, label_stringIdx)\n",
    "    elif featureExtractor==\"tfidf\":\n",
    "        pipeline = tfidf(regexTokenizer, stopwordsRemover, label_stringIdx)\n",
    "    print(\"Pipeline constructed successfully..\")\n",
    "\n",
    "    # Fit the pipeline to the model.\n",
    "    pipelineFit = pipeline.fit(data)\n",
    "    \n",
    "    dataset = pipelineFit.transform(data)\n",
    "    # Part 5 -> split data and classify\n",
    "    (trainingData, validationData, testData) = dataset.randomSplit([0.8, 0.1, 0.1], seed = 100)\n",
    "    \n",
    "    \n",
    "    print(\"Training model..\")\n",
    "    if classifier==\"lr\":\n",
    "        selectedModel = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "    elif classifier==\"nb\":\n",
    "        selectedModel = NaiveBayes(smoothing=1)\n",
    "    elif classifier==\"rf\":\n",
    "        selectedModel = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 20, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "    model = selectedModel.fit(trainingData)\n",
    "    print(\"Model Trained..\")\n",
    "    \n",
    "    # part 6 -> predict and test model\n",
    "    print(\"Test model..\")\n",
    "    predictions = model.transform(validationData)\n",
    "    predictions.filter(predictions['prediction'] == 0) \\\n",
    "        .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "        .orderBy(\"probability\", ascending=False)\n",
    "\n",
    "    print(\"Evaluate model..\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy: \"+str(accuracy))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Please select method for feature extraction. Type\\ncv for count vectorizer\\ntfidf for term \\\n",
    "frequency inverse document frequency\")\n",
    "    fe_choice = input()\n",
    "    print(\"\\nPlese enter algorithm for classification. Type\\nlr for linear regression\\nnb for naive bayes\\n\\\n",
    "rf for random forest\")\n",
    "    algo_choice = input()\n",
    "    classify(fe_choice,algo_choice)\n",
    "    sc.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select method for feature extraction. Type\n",
      "cv for count vectorizer\n",
      "tfidf for term frequency inverse document frequency\n",
      "tfidf\n",
      "\n",
      "Plese enter algorithm for classification. Type\n",
      "lr for linear regression\n",
      "nb for naive bayes\n",
      "rf for random forest\n",
      "nb\n",
      "Data Read successfully ...\n",
      "Data Read successfully ...\n",
      "Data Read successfully ...\n",
      "Data Read successfully ...\n",
      "The total number of articles from 4 categories is  243\n",
      "Pipeline constructed successfully..\n",
      "Training model..\n",
      "Model Trained..\n",
      "Test model..\n",
      "Evaluate model..\n",
      "Accuracy: 0.6915151515151515\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cv    tfidf\n",
    "lr 0.631  0.733\n",
    "nb 0.654  0.692\n",
    "rf 0.737  0.732"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
